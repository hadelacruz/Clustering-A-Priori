---
title: "Laboratorio 2"
author: "Humberto de la Cruz, Dilary Cruz, Daniel Juárez"
date: "2026-02-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r preprocesamiento, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
# =====================================================
# Carga de librerías y datos
# =====================================================
# Nota: Asegúrate de tener instalado fastDummies en tu consola, 
# no uses install.packages() dentro del Rmd.
library(dplyr)
library(tidyr)
library(stringr)
library(fastDummies)
library(arules)
library(arulesViz)



movies <- read.csv("movies_2026.csv", header = TRUE)
movies_clean <- movies

# =====================================================
# 1. Eliminar columnas que no aportan al clustering
# =====================================================
movies_clean <- movies_clean %>%
  select(-id, -originalTitle, -title, -homePage, -actorsCharacter,
         -releaseDate, -productionCompany, -productionCompanyCountry,
         -actors, -actorsPopularity, -productionCountry, -director,
         -originalLanguage, -video, -genres )

# =====================================================
# 2. Crear variables agregadas de actorsPopularity
# =====================================================
actor_pop_list <- strsplit(movies$actorsPopularity, "\\|")

movies_clean$mean_actor_popularity <- sapply(actor_pop_list, function(x) {
  vals <- as.numeric(x)
  vals <- vals[!is.na(vals)]
  if(length(vals) == 0) return(NA)
  mean(vals)
})

movies_clean$max_actor_popularity <- sapply(actor_pop_list, function(x) {
  vals <- as.numeric(x)
  vals <- vals[!is.na(vals)]
  if(length(vals) == 0) return(NA)
  max(vals)
})

# =====================================================
# 3. Transformación logarítmica y filtros
# =====================================================
movies_clean <- movies_clean %>%
  mutate(
    budget_log  = log1p(budget),
    revenue_log = log1p(revenue)
  ) %>%
  select(-budget, -revenue) %>%
  # Es mejor filtrar anomalías ANTES de escalar
  filter(voteCount > 0, 
         voteAvg > 0, 
         budget_log > 0, 
         revenue_log > 0)


# =====================================================
# 4. Crear variables dummy para los géneros más frecuentes
# =====================================================

# Eliminar columna original de géneros
#movies_clean <- movies_clean %>% select(-genres)


# =====================================================
# 5. Limpieza de NAs y Escalamiento (Crucial)
# =====================================================
movies_clean <- na.omit(movies_clean)

# Escalar y convertir a data frame
movies_scaled <- as.data.frame(scale(movies_clean))

# Verificar resultado
dim(movies_scaled)
summary(movies_scaled)

```



## 1. Clustering

### 1.2 Tendencia al Agrupamiento (Hopkins y VAT)

```{r punto-1-2, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5, results='hide'}
library(factoextra)
library(clustertend)

# Tomamos una muestra de 1000 observaciones para no saturar la memoria RAM
# al calcular la matriz de distancias para el VAT
set.seed(23709) 
muestra_idx <- sample(1:nrow(movies_scaled), 1000)
movies_muestra <- movies_scaled[muestra_idx, ]

# Estadístico de Hopkins
hopkins_stat <- hopkins(movies_muestra, n = nrow(movies_muestra) - 1)
cat("Estadístico de Hopkins:", hopkins_stat$H, "\n")

# Gráfica VAT
fviz_dist(dist(movies_muestra), show_labels = FALSE) +
  labs(title = "VAT - Visual Assessment of cluster Tendency")
```


```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
fviz_nbclust(
  movies_scaled,
  FUNcluster = function(x, k){
    kmeans(x, centers = k, nstart = 25, iter.max = 100)
  },
  method = "wss",
  k.max = 10
) +
labs(
  title = "Metodo del Codo para determinar k optimo",
  x = "Numero de Clusters k",
  y = "Suma de Cuadrados Intra-grupo (WSS)"
)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
k_optimo <- 2

# 1. K-Medias
set.seed(23709)
kmeans_res <- kmeans(movies_scaled, centers = k_optimo, nstart = 25)

# 2. Clustering Jerárquico (Usando la muestra por capacidad de cómputo)
matriz_distancias <- dist(movies_muestra)
hc_res <- hclust(matriz_distancias, method = "ward.D")
hc_grupos <- cutree(hc_res, k = k_optimo)

# Comparación visual (K-medias)
fviz_cluster(kmeans_res, data = movies_scaled, 
             geom = "point", ellipse.type = "convex", 
             show.clust.cent = FALSE, labelsize = 0,
             main = "Agrupamiento K-Medias")
```

```{r,message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
library(cluster)

# Silueta para K-medias (sobre una muestra representativa)
sil_kmeans <- silhouette(kmeans_res$cluster[muestra_idx], dist(movies_muestra))
fviz_silhouette(sil_kmeans, print.summary = FALSE) + 
  labs(title = "Silueta para K-Medias (k=2)")

# Silueta para Jerárquico
sil_hc <- silhouette(hc_grupos, matriz_distancias)
fviz_silhouette(sil_hc, print.summary = FALSE) + 
  labs(title = "Silueta para Jerarquico (k=2)")
```



```{r,message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# Agregar el cluster de k-medias al dataset limpio original para interpretar
movies_analisis <- movies_clean %>%
  mutate(Cluster = as.factor(kmeans_res$cluster))

# Medidas de tendencia central por grupo (Variables continuas)
resumen_grupos <- movies_analisis %>%
  group_by(Cluster) %>%
  summarise(
    Promedio_Popularidad = mean(popularity),
    Promedio_Votos_Avg = mean(voteAvg),
    Promedio_Duracion = mean(runtime),
    Promedio_Ingresos_Log = mean(revenue_log),
    Cant_Peliculas = n()
  )

knitr::kable(resumen_grupos, digits = 2, caption = "Perfil Promedio por Clúster")
```


## 2. Análisis de Componentes Principales (PCA)

### 2.1. Transformación de variables categóricas
**¿Es posible y vale la pena?**

Técnicamente es posible transformar variables categóricas utilizando variables *dummy* (0 y 1) sin embargo, en nuestro dataset de películas, estas variables presentan una alta cardinalidad al convertirlas, crearíamos una cantidad excesiva de columnas nuevas, generando un encuadre muy disperso que diluiría la varianza real que buscamos capturar por lo tanto, determinamos que **no vale la pena** incluirlas.


### 2.2. Conveniencia del Análisis (Matriz, KMO y Bartlett)

Primero, preparamos nuestras variables numéricas y calculamos las pruebas de factibilidad para asegurarnos de que el lienzo de datos es el adecuado.

```{r pca-pruebas, message=FALSE, warning=FALSE,echo=FALSE,results='hide'}
library(psych)
library(corrplot)
library(factoextra)

# Seleccionamos las variables numéricas limpias, evitando multicolinealidad perfecta
numericas_pca <- movies_clean %>%
  select(popularity, runtime, voteCount, voteAvg, actorsAmount, 
         genresAmount, productionCoAmount, productionCountriesAmount, 
         budget_log, revenue_log, mean_actor_popularity, max_actor_popularity, releaseYear) %>%
  na.omit()

# 1. Matriz de Correlación
matriz_cor <- cor(numericas_pca)
corrplot(matriz_cor, method = "color", type = "upper", tl.cex = 0.7)

# 2. Prueba de KMO
KMO(matriz_cor)

# 3. Test de Esfericidad de Bartlett
cortest.bartlett(matriz_cor, n = nrow(numericas_pca))
```

**Interpretación de los resultados y viabilidad:**

**Estudio de la Matriz de Correlación:** 
Al observar el gráfico, notamos inmediatamente "bloques" de color azul oscuro que nos indican fuertes correlaciones positivas. Destaca la altísima relación entre el presupuesto (`budget_log`) y los ingresos (`revenue_log`), los cuales a su vez están fuertemente ligados a la cantidad de votos (`voteCount`). También vemos una clara conexión entre la popularidad media y máxima de los actores.

* **Técnica de Análisis Factorial (KMO):** 
Obtuvimos un índice KMO general de **0.6**. Como este valor alcanza el umbral mínimo aceptable, concluimos que la proporción de varianza es adecuada y **sí es posible** utilizar la técnica de análisis factorial para hallar las componentes principales

* **Prueba de Esfericidad de Bartlett:** 
El valor p obtenido es **0** (prácticamente cero absoluto), el cual es muchísimo menor a 0.05. Esto nos permite rechazar con total seguridad la hipótesis nula de que la matriz es una matriz identidad, confirmando matemáticamente la fuerte multicolinealidad observada en la gráfica. En definitiva, **sí vale la pena** aplicar las componentes principales.

---

### 2.3. Ejecución del PCA e Interpretación

A continuación, estandarizamos los datos directamente en la función y ejecutamos el modelo para extraer nuestros componentes principales.

```{r pca-modelo,message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# Ejecución del modelo PCA
pca_result <- prcomp(numericas_pca, center = TRUE, scale. = TRUE)

# Resumen de la variabilidad explicada
summary(pca_result)

# Gráfico de sedimentación
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))

# Coeficientes principales (Loadings) de los primeros componentes
round(pca_result$rotation[, 1:4], 3)
```

**Selección de Componentes (Variabilidad):**

Al observar el gráfico de sedimentación (*Scree plot*), buscamos el punto donde la caída de la varianza crea un "codo" y comienza a estabilizarse. Vemos que el mayor aporte de información se da al inicio. Por lo tanto **seleccionaremos los primeros 4 componentes**. 
Tomamos esta decisión porque al sumar sus varianzas individuales estos 4 ejes logran explicar de forma acumulada el **57.2% de la variabilidad total** de nuestros datos, logrando un excelente equilibrio entre reducir la dimensionalidad y retener la información más valiosa de las películas.


**Interpretación de los Coeficientes Principales:**
Analizando la matriz de rotación y basándonos en las correlaciones previas, estos nuevos "ángulos" de nuestros datos se interpretan de la siguiente manera:

* **Componente 1 (Dim1 - Éxito Comercial e Inversión):** 

Este componente lidera con el 22.9% de la variabilidad y está fuertemente dibujado por las variables financieras (`budget_log`, `revenue_log`) y de interacción del público (`voteCount`). Nos habla directamente de la magnitud económica y el impacto comercial de la cinta.

* **Componente 2 (Dim2 - Poder Estelar del Elenco):**

Con un 14.7%, este eje captura casi en su totalidad el peso de las variables `mean_actor_popularity` y `max_actor_popularity`. Es un indicador claro y directo de qué tan famoso y reconocido es el talento involucrado en la producción.

* **Componentes 3 y 4 (Estructura de Producción):** 

Estos componentes recogen las varianzas secundarias relacionadas con la logística y el formato, balanceando variables como la cantidad total de actores en el reparto (`actorsAmount`), la duración de la película (`runtime`) y la cantidad de compañías productoras involucradas.



## 3. Reglas de Asociación – Algoritmo A priori

```{r, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}
datos_reglas <- movies_analisis %>%
  select(popularity, voteAvg, runtime, budget_log, revenue_log, Cluster)
```


```{r, message=FALSE, echo=FALSE, warning=FALSE, results='hide'}

datos_reglas$popularity <- discretize(datos_reglas$popularity, 
                                       method = "frequency", categories = 3)

datos_reglas$voteAvg <- discretize(datos_reglas$voteAvg, 
                                   method = "frequency", categories = 3)

datos_reglas$runtime <- discretize(datos_reglas$runtime, 
                                   method = "frequency", categories = 3)

datos_reglas$budget_log <- discretize(datos_reglas$budget_log, 
                                      method = "frequency", categories = 3)

datos_reglas$revenue_log <- discretize(datos_reglas$revenue_log, 
                                       method = "frequency", categories = 3)

datos_reglas$Cluster <- as.factor(datos_reglas$Cluster)
```

```{r , message=FALSE, echo=FALSE, warning=FALSE, results='hide'}

transacciones <- as(datos_reglas, "transactions")

```


### 3.1 Metodología

Para identificar patrones frecuentes entre las características de las películas, se aplicó el algoritmo **A priori**.  
Dado que el algoritmo requiere variables categóricas, las variables numéricas fueron discretizadas en tres niveles (bajo, medio, alto) utilizando el método de frecuencia (cuantiles).

Se incluyeron las variables:

- Popularidad  
- Promedio de votos  
- Duración  
- Presupuesto (log)  
- Ingresos (log)  
- Clúster obtenido por K-medias  

Se probaron tres configuraciones de soporte y confianza con el fin de evaluar la estabilidad y relevancia de las reglas obtenidas.

---

### 3.2 Resultados con Soporte Alto (0.10)
```{r , message=FALSE, echo=FALSE, warning=FALSE}

reglas1 <- apriori(transacciones,
                   parameter = list(supp = 0.1, conf = 0.7, minlen = 2))

summary(reglas1)
inspect(head(sort(reglas1, by = "lift"), 10))

```

Con soporte mínimo de 10% se generaron 53 reglas.  
Estas reglas representan patrones generales del dataset.

Se observa que combinaciones de:

- Presupuesto alto  
- Ingresos altos  
- Mayor duración  

están fuertemente asociadas al **Cluster 2**, con valores de lift superiores a 2.5.  

Esto indica que dichas características aumentan significativamente la probabilidad de pertenecer a ese grupo.

---

### 3.3 Resultados con Soporte Medio (0.05)
```{r , message=FALSE, echo=FALSE, warning=FALSE}

reglas2 <- apriori(transacciones,
                   parameter = list(supp = 0.05, conf = 0.6, minlen = 2))

summary(reglas2)
inspect(head(sort(reglas2, by = "lift"), 10))

```

Al reducir el soporte al 5%, se obtuvieron 265 reglas, más específicas y con mayor nivel de detalle.

Aparecen reglas con:

- Confianza cercana a 0.98  
- Lift alrededor de 3  

Esto significa que ciertas combinaciones (alta popularidad, alto presupuesto y altos ingresos) hacen que la pertenencia al Cluster 2 sea hasta tres veces más probable que en el promedio general.

---

### 3.4 Resultados con Soporte Bajo (0.02)
```{r , message=FALSE, echo=FALSE, warning=FALSE}

reglas3 <- apriori(transacciones,
                   parameter = list(supp = 0.02, conf = 0.5, minlen = 2))

summary(reglas3)
inspect(head(sort(reglas3, by = "lift"), 10))

```

Con soporte de 2% se generaron 960 reglas.  
Aunque algunas presentan confianza igual a 1, su cobertura es baja, lo que indica que describen casos muy específicos.

A menor soporte aumenta el número de reglas, pero disminuye su capacidad de generalización.

---

### 3.5 Discusión General

En todos los niveles de soporte se observa un patrón consistente:  

El **Cluster 2** está asociado con películas de:

- Alta popularidad  
- Alto presupuesto  
- Altos ingresos  
- Mayor duración  

Esto confirma los resultados obtenidos previamente en el análisis de clustering y PCA, evidenciando que dicho clúster representa películas de mayor escala comercial.

Es importante señalar que las reglas de asociación muestran relaciones probabilísticas frecuentes, pero no implican causalidad.

---

## 4. Otros algoritmos de aprendizaje no supervisado (t-SNE)

### 4.1. Elección y justificación del algoritmo

Para esta sección hemos decidido implementar el algoritmo **t-SNE** (t-Distributed Stochastic Neighbor Embedding). 

**¿Por qué t-SNE para este conjunto de datos?**

Mientras que el Análisis de Componentes Principales (PCA) que realizamos anteriormente es una técnica lineal excelente para preservar la estructura global y las grandes varianzas del dataset, t-SNE es una técnica no lineal especializada en preservar la estructura *local*. 

En el contexto de la industria las películas pueden tener relaciones complejas y no lineales t-SNE es de extrema utilidad aquí porque calcula la probabilidad de que dos películas sean "vecinas" en el espacio multidimensional y las proyecta en un mapa 2D. Esto nos permitirá descubrir si existen sub-agrupaciones más finas, nichos muy específicos o "islas" de películas que K-Medias y PCA agruparon de forma muy general.

---

### 4.2. Ejecución e Interpretación de Resultados

Procedemos a ejecutar el algoritmo sobre nuestras variables numéricas limpias

```{r tsne-ejecucion, message=FALSE, warning=FALSE, echo=FALSE}
# Cargar librería necesaria (Asegurarse de tenerla instalada en la consola)
library(Rtsne)
library(ggplot2)

# t-SNE requiere que no existan filas exactamente duplicadas. 
# Filtramos los datos numéricos limpios por seguridad.
datos_tsne <- numericas_pca[!duplicated(numericas_pca), ]

set.seed(23709)

# Ejecutamos t-SNE
# perplexity = 30 
tsne_out <- Rtsne(datos_tsne, dims = 2, perplexity = 30, verbose = FALSE, max_iter = 500)

# Creamos un data frame con los resultados 2D para graficar
tsne_df <- data.frame(
  Dim1 = tsne_out$Y[, 1],
  Dim2 = tsne_out$Y[, 2]
)

# Graficamos el resultado
ggplot(tsne_df, aes(x = Dim1, y = Dim2)) +
  geom_point(alpha = 0.4, color = "#2c3e50", size = 1.5) +
  theme_minimal() +
  labs(title = "Mapa de Similitud de Películas (t-SNE)",
       subtitle = "Proyección no lineal de las variables numéricas",
       x = "Dimensión t-SNE 1",
       y = "Dimensión t-SNE 2")
```

**Interpretación y relevancia de los hallazgos:**

* **Agrupación Visual:** Al observar el mapa generado por t-SNE, el hallazgo más sorprendente es que las películas no forman simples nubes aleatorias, sino que se organizan en estructuras alargadas, curvas y sumamente definidas cada uno de estos caminos representa secuencias y nichos de películas con perfiles estadísticos casi idénticos, demostrando que existen "familias" de producciones con características muy particulares de inversión, popularidad y elenco.


* **Transiciones y Espectro:** A diferencia de una clasificación rígida el gráfico muestra carriles continuos separados por vacíos muy claros. Esto nos indica que el mercado cinematográfico sigue patrones de progresión marcados. Las películas experimentan una transición fluida dentro de su propio camino pero existen fronteras o vacíos que separan categóricamente a un tipo de producción de otra


* **Complemento al Clustering Previo:** Mientras que el algoritmo K-medias nos dio una visión macroscópica t-SNE nos revela la verdadera topología compleja y no lineal de los datos. Demuestra que la industria no se divide simplemente en "películas grandes vs. pequeñas", sino en múltiples micro-estructuras y trayectorias de similitud que K-medias no puede detectar.


